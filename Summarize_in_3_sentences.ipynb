{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNnAhoDx8pFjHr2wr9f4y6k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jkhyjkhy/LeetCode/blob/main/Summarize_in_3_sentences.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "0f72BNtE_VRk"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup as bs\n",
        "import re\n",
        "import pandas as pd\n",
        "import math\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import normalize\n",
        "from _datetime import datetime\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_soup(url): # soup 객체를 가져옴\n",
        "    res = requests.get(url)\n",
        "    if res.status_code == 200:\n",
        "        return bs(res.text, 'html.parser')\n",
        "    else:\n",
        "        print(f\"Super big fail! with {res.status_code}\")\n"
      ],
      "metadata": {
        "id": "bQ3ZX5jL_iqa"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_head(soup):\n",
        "    head = soup.find('div', attrs={\"class\": \"news_headline\"})\n",
        "    date_info = head.find('div', attrs={\"class\": \"info\"})\n",
        "\n",
        "    print(\"기사 제목 : \" + head.select_one('.title').get_text())\n",
        "    print(\"기사 작성 시간 : \" + date_info.select_one('span').get_text())\n",
        "    # print(\"기자 이름 : \" + head.find('em', attrs ={\"class\" : \"media_end_head_journalist_name\"}).get_text()[0:3]) , 적용이 안되는 기자가 많아 예외 처리\n"
      ],
      "metadata": {
        "id": "8T4_uzRM_pJ9"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_content(soup):\n",
        "    article = soup.find('div', attrs={\"id\": \"newsEndContents\"})\n",
        "\n",
        "    content = article.text.replace(\"\\n\",\"\")\n",
        "    content = re.split(\"[\\.?!]\\s+\", content) # 문장을 요소 단위로 분화, 문장 구분\n",
        "\n",
        "\n",
        "    return content"
      ],
      "metadata": {
        "id": "2OMYmOao_jxF"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_list(url): # 이 시각 많이 본 뉴스에서 리스트를 가져옴\n",
        "    print(f\"현재 시간은 {datetime.now()}입니다. 상위 10개의 기사를 조회합니다.\")\n",
        "    soup = get_soup(url)\n",
        "    # news_list = soup.find('ol', attrs={\"class\" : \"news_list\"})\n",
        "    news_list = soup.select('.news_list') # select를 써 본 기억이 없는것 같아 select로 구현\n",
        "    news_link = {} # 딕셔너리로 선언\n",
        "    for li in range(0, 10):\n",
        "        print(f\"{li+1}번 기사 : {news_list[0].findAll('a')[li].get_text()}\")\n",
        "        news_link[li] = news_list[0].findAll('a')[li].get('href')\n",
        "\n",
        "    print(\"================================\")\n",
        "\n",
        "    checking_number = int(input(\"조회하고 싶은 기사를 입력하세요 : \")) # 조회를 원하는 기사 본문의 url 저장\n",
        "\n",
        "    return \"https://sports.news.naver.com/\" + news_link[checking_number-1]"
      ],
      "metadata": {
        "id": "rxnouaA2_bA1"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def textrank(x, df=0.85, max_iter=50): # df = Dumping Factor : 0.85라고 가정, max_iter = 최대 반복횟수, 제한 조건을 두고 적절한 값에 수렴할떄까지 반복해야 하나 임의적으로 50회라고 지정\n",
        "    assert 0 < df < 1\n",
        "\n",
        "    # initialize\n",
        "    A = normalize(x, axis=0, norm='l1')\n",
        "    R = np.ones(A.shape[0]).reshape(-1, 1)\n",
        "    bias = (1 - df) * np.ones(A.shape[0]).reshape(-1, 1)\n",
        "    # iteration\n",
        "    for _ in range(max_iter):\n",
        "        R = df * (A * R) + bias # @ 연산으로 한번에 계산해도 됨\n",
        "\n",
        "    return R"
      ],
      "metadata": {
        "id": "g9z-lMB4_Xh7"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_text(content):\n",
        "    data = []\n",
        "    for text in content:\n",
        "        if (text == \"\" or len(text) == 0):\n",
        "            continue\n",
        "        elif text == \"All right reserved\": # 기사가 끝났으면 반복문 종료\n",
        "            break\n",
        "        temp_dict = dict()\n",
        "        temp_dict['sentence'] = text\n",
        "        temp_dict['token_list'] = text.split()  # 기초적인 띄어쓰기 단위로 나누기\n",
        "        # 한국어 전처리 분석기인 꼬꼬마 분석기 사용도 고려해봐야 할 것 같음\n",
        "        data.append(temp_dict)\n",
        "\n",
        "    data_frame = pd.DataFrame(data)\n",
        "    # print(data_frame) # 문장 리스트와 토큰화 된걸 보려면 주석 해제할 것\n",
        "    print(\"================================\")\n",
        "\n",
        "    # 여기서부터\n",
        "    # reference : https://hoonzi-text.tistory.com/68, 문서 요약 하기 (with textrank)\n",
        "    # reference2 : https://lovit.github.io/nlp/2019/04/30/textrank/, TextRank 를 이용한 키워드 추출과 핵심 문장 추출 (구현과 실험)\n",
        "\n",
        "    similarity_matrix = []\n",
        "    for i, row_i in data_frame.iterrows():\n",
        "        i_row_vec = []\n",
        "        for j, row_j in data_frame.iterrows():\n",
        "            if i == j:\n",
        "                i_row_vec.append(0.0)\n",
        "            else:\n",
        "                intersection = len(set(row_i['token_list']) & set(row_j['token_list'])) # 유사도 계산의 분자 부분\n",
        "                log_i = math.log(len(set(row_i['token_list'])))\n",
        "                log_j = math.log(len(set(row_j['token_list'])))\n",
        "                similarity = intersection / (log_i + log_j)\n",
        "                i_row_vec.append(similarity)\n",
        "\n",
        "        similarity_matrix.append(i_row_vec)\n",
        "\n",
        "        weightedGraph = np.array(similarity_matrix)\n",
        "\n",
        "    R = textrank(weightedGraph)\n",
        "    R = R.sum(axis=1)\n",
        "\n",
        "    indexs = R.argsort()[-3:] # 랭크값 상위 세 문장의 인덱스를 가져옴\n",
        "    for index in sorted(indexs): # 뉴스 구조의 순서를 유지하기 위해 등장한 순서대로 정렬함\n",
        "        print(data_frame['sentence'][index])\n",
        "        print()\n"
      ],
      "metadata": {
        "id": "xs07tWoh_zdf"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    url = get_list(\"https://sports.news.naver.com/wfootball/index\")\n",
        "    soup = get_soup(url)\n",
        "    get_head(soup)\n",
        "    print(\"================================\")\n",
        "    get_content(soup)\n",
        "    summarize_text(get_content(soup))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1NZIhnQ3_2y7",
        "outputId": "82ba4f76-3395-42fc-d21b-db0aaa9afed8"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "현재 시간은 2023-11-05 15:21:38.188174입니다. 상위 10개의 기사를 조회합니다.\n",
            "1번 기사 : 재계약 거부하고 이적 결심...토트넘 보강 계획 ‘청신호’ 잡혀\n",
            "2번 기사 : \"SON, 골 좀 그만 넣어\"…아스널 DF, 퀴즈쇼에서 '토트넘 캡틴' 손흥민 극찬\n",
            "3번 기사 : ‘이강인 멀티포지션 능력 덕분에 뎀벨레까지 살아났다!’ 엔리케 감독 함박웃음 지은 이유\n",
            "4번 기사 : '내 뒤에 부폰 있다!'…'요리스 삭제해 버린' 토트넘 신입 GK, 다 이유가 있었네! \"부폰이 항상 가치 있는 조언 해줘\"\n",
            "5번 기사 : \"이강인 처음봤을 때 놀랐다\" 700억 수비수 극찬…프랑스 매체들도 호평\n",
            "6번 기사 : 이대로 가면 51골! '美친 골감각' 케인, 레반도프스키 41골 기록 넘어 '역대 최다골 득점왕' 가능성 ↑\n",
            "7번 기사 : '포버지'가 밝힌 '매디슨X비카리오X판 더 펜 영입 성공 비결'...\"선수가 아닌 사람으로 본다\"\n",
            "8번 기사 : 손흥민 어떻게 잊나…\"포체티노 토트넘 복귀 원했다\"\n",
            "9번 기사 : 첼시 20살 DF의 도발 \"토트넘이 우릴 언제 이겼더라? 기억 안 나\"\n",
            "10번 기사 : 무대 옮겨도 잘한다...케인, '첫 10경기 15골' 분데스 역사상 최다 득점 신기록+첫 데어클라시커 해트트릭 주인공\n",
            "================================\n",
            "조회하고 싶은 기사를 입력하세요 : 8\n",
            "기사 제목 : 손흥민 어떻게 잊나…\"포체티노 토트넘 복귀 원했다\"\n",
            "기사 작성 시간 : 기사입력 2023.11.05. 오후 09:41\n",
            "================================\n",
            "================================\n",
            "▲ 마우리시오 포체티노 감독은 손흥민을 주축 전력으로 활용했다.[스포티비뉴스=김건일 기자] 마우리시오 포체티노 첼시 감독이 토트넘 홋스퍼로 복귀를 희망했던 것으로 드러났다.영국 데일리메일 사미 목벨 기자는 5일(한국시간) 토트넘과 포체티노 감독의 관계를 조명하며 이같이 밝혔다.목벨 기자에 따르면 한 소식통은 \"토트넘에서 경질은 포체티노 감독 인생에서 가장 가슴 아픈 일이었다\n",
            "\n",
            "이후 포체티노 감독은 2022년 1월 파리생제르맹 감독으로 부임한 뒤 이번 시즌을 앞두고 첼시 감독으로 선임되어 프리미어리그로 돌아왔다.토트넘에서 큰 성공으로 포체티노 감독은 꾸준히 토트넘과 연결됐다\n",
            "\n",
            "일부 토트넘 선수 및 스태프도 포체티노 감독 복귀를 요구한 것으로 알려졌다\n",
            "\n"
          ]
        }
      ]
    }
  ]
}